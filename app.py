import os
import glob
import streamlit as st
from dotenv import load_dotenv
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain.vectorstores import FAISS
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser

# â”€â”€â”€ PAGE CONFIG â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.set_page_config(
    page_title="ForteBank RAG Chatbot",
    page_icon="ğŸ¦",
    layout="wide",
)

# â”€â”€â”€ ENVIRONMENT â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Load .env if present
load_dotenv()
OPENAI_API_KEY = st.secrets.get("OPENAI_API_KEY", os.getenv("OPENAI_API_KEY"))
if not OPENAI_API_KEY:
    st.error("âŒ OPENAI_API_KEY Ğ½Ğµ Ğ·Ğ°Ğ´Ğ°Ğ½Ğ°.")
    st.stop()

# Path to your committed PDFs
DATA_PATH = "./knowledge_base"

# â”€â”€â”€ RAG PROMPT TEMPLATE â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
RAG_TEMPLATE = """Ğ¢Ñ‹ â€” Ğ´Ñ€ÑƒĞ¶ĞµĞ»ÑĞ±Ğ½Ñ‹Ğ¹ Ğ¸ Ğ¿Ñ€Ğ¾Ñ„ĞµÑÑĞ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ°ÑÑĞ¸ÑÑ‚ĞµĞ½Ñ‚ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ğ»Ğ°Ñ‚ĞµĞ¶Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼ ForteBank. 
Ğ”Ğ°Ğ¹ Ğ¾Ñ‚Ğ²ĞµÑ‚ Ğ½Ğ° Ğ²Ğ¾Ğ¿Ñ€Ğ¾Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑĞ»ĞµĞ´ÑƒÑÑ‰ĞµĞ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°:
{context}

# Ğ˜Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸
- ĞÑ‚Ğ²ĞµÑ‡Ğ°Ğ¹ Ğ½Ğ° Ñ€ÑƒÑÑĞºĞ¾Ğ¼.
- ĞŸĞ¾Ğ·Ğ´Ğ¾Ñ€Ğ¾Ğ²Ğ°Ğ¹ÑÑ Ğ¸ Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ¾Ñ‚Ğ²ĞµÑ‡Ğ°Ğ¹.
- Ğ•ÑĞ»Ğ¸ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ ÑĞ²Ğ½Ñ‹Ğ¹ Ğ¾Ñ‚Ğ²ĞµÑ‚ â€” Ğ¿Ñ€Ğ¸Ğ²ĞµĞ´Ğ¸ ĞµĞ³Ğ¾.
- Ğ•ÑĞ»Ğ¸ Ğ¿Ğ¾Ñ…Ğ¾Ğ¶Ğ¸Ğ¹ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ â€” Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸ ÑƒÑ‚Ğ¾Ñ‡Ğ½Ğ¸Ñ‚ÑŒ.
- Ğ•ÑĞ»Ğ¸ Ğ½ĞµÑ‚ â€” Ğ²ĞµĞ¶Ğ»Ğ¸Ğ²Ğ¾ Ğ¿ĞµÑ€ĞµĞºĞ»ÑÑ‡Ğ¸ Ğ½Ğ° ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸ÑÑ‚Ğ°.

Ğ’Ğ¾Ğ¿Ñ€Ğ¾Ñ: {question}
"""

# â”€â”€â”€ CACHED RESOURCES â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@st.cache_resource
def get_embedding_function():
    return OpenAIEmbeddings(
        model="text-embedding-3-small",
        openai_api_key=OPENAI_API_KEY
    )

@st.cache_resource
def build_vector_store(_docs):
    """Split â†’ embed â†’ store entirely in RAM (no disk)."""
    splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
    chunks = splitter.split_documents(_docs)
    texts = [chunk.page_content for chunk in chunks]
    metadatas = [chunk.metadata for chunk in chunks]
    embed_fn = get_embedding_function()
    return FAISS.from_texts(
        texts,
        embed_fn,
        metadatas=metadatas
    )

@st.cache_resource(hash_funcs={FAISS: lambda _: None})
def create_rag_chain(vector_store, 'gpt-4o-mini', temperature=0):
    llm = ChatOpenAI(
        model=model_name,
        temperature=temperature,
        openai_api_key=OPENAI_API_KEY
    )
    retriever = vector_store.as_retriever(
        search_type="similarity",
        search_kwargs={"k": 3}
    )
    prompt = ChatPromptTemplate.from_template(RAG_TEMPLATE)
    return (
        {"context": retriever, "question": RunnablePassthrough()}
        | prompt
        | llm
        | StrOutputParser()
    )

# â”€â”€â”€ HELPERS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def load_local_docs():
    """Load all PDFs from your committed folder."""
    paths = glob.glob(os.path.join(DATA_PATH, "**/*.pdf"), recursive=True)
    docs = []
    for p in paths:
        docs.extend(PyPDFLoader(p).load())
    return docs

# â”€â”€â”€ MAIN â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def main():
    st.title("ğŸ¦ ForteBank RAG Chatbot")
    st.markdown("Ğ—Ğ°Ğ´Ğ°Ğ¹Ñ‚Ğµ Ğ²Ğ¾Ğ¿Ñ€Ğ¾Ñ Ğ¿Ğ¾ Ğ¿Ğ»Ğ°Ñ‚ĞµĞ¶Ğ½Ñ‹Ğ¼ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ğ¼ ForteBank.")

    # â”€ List indexed PDFs â”€
    st.sidebar.markdown("---")
    st.sidebar.header("ğŸ“„ Ğ˜Ğ½Ğ´ĞµĞºÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ PDF")
    pdfs = glob.glob(os.path.join(DATA_PATH, "**/*.pdf"), recursive=True)
    if pdfs:
        for pdf in pdfs:
            rel = os.path.relpath(pdf, DATA_PATH)
            st.sidebar.text(f"â€¢ {rel}")
    else:
        st.sidebar.text("â€” ĞĞµÑ‚ PDF-Ñ„Ğ°Ğ¹Ğ»Ğ¾Ğ² â€”")

    # â”€ Upload new PDFs â”€
    st.sidebar.markdown("---")
    st.sidebar.header("ğŸ“¤ Ğ—Ğ°Ğ³Ñ€ÑƒĞ·Ğ¸Ñ‚ÑŒ PDF")
    uploaded = st.sidebar.file_uploader(
        "Ğ’Ñ‹Ğ±ĞµÑ€Ğ¸Ñ‚Ğµ PDF-Ñ„Ğ°Ğ¹Ğ»Ñ‹",
        type=["pdf"],
        accept_multiple_files=True,
        key="pdf_uploader"
    )
    if uploaded and not st.session_state.get("upload_processed", False):
        new_docs = []
        for f in uploaded:
            new_docs.extend(PyPDFLoader(f).load())
        combined = load_local_docs() + new_docs
        st.session_state.vector_store = build_vector_store(combined)
        st.session_state.upload_processed = True
        st.sidebar.success("âœ… Ğ—Ğ°Ğ³Ñ€ÑƒĞ¶ĞµĞ½Ğ¾ Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ½Ğ´ĞµĞºÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¾!")

    # â”€ Initialize vector store â”€
    if "vector_store" not in st.session_state:
        st.session_state.vector_store = build_vector_store(load_local_docs())

    # â”€ Build RAG chain â”€
    vector_store = st.session_state.vector_store
    rag_chain = create_rag_chain(vector_store, model_option, temperature)

    # â”€ Chat UI â”€
    if "messages" not in st.session_state:
        st.session_state.messages = []

    for msg in st.session_state.messages:
        with st.chat_message(msg["role"]):
            st.markdown(msg["content"])

    if prompt := st.chat_input("Ğ’Ğ°Ñˆ Ğ²Ğ¾Ğ¿Ñ€Ğ¾Ñ..."):
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)
        with st.chat_message("assistant"):
            with st.spinner("ĞĞ±Ğ´ÑƒĞ¼Ñ‹Ğ²Ğ°Ñ Ğ¾Ñ‚Ğ²ĞµÑ‚â€¦"):
                try:
                    answer = rag_chain.invoke(prompt)
                except Exception as e:
                    answer = f"ĞÑˆĞ¸Ğ±ĞºĞ°: {e}"
                st.markdown(answer)
                st.session_state.messages.append({"role": "assistant", "content": answer})

    if st.button("ğŸ—‘ï¸ ĞÑ‡Ğ¸ÑÑ‚Ğ¸Ñ‚ÑŒ Ñ‡Ğ°Ñ‚"):
        st.session_state.messages = []

if __name__ == "__main__":
    main()
