import sys
# 1) Load the drop-in sqlite3 substitute...
import pysqlite3
# 2) And override the stdlib name so further 'import sqlite3' uses it
sys.modules["sqlite3"] = pysqlite3

import os
os.environ["PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION"] = "python"

import os
import glob
import streamlit as st
from dotenv import load_dotenv
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_community.vectorstores import Chroma
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
from chromadb.config import Settings


# â”€â”€â”€ PAGE CONFIG â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.set_page_config(
    page_title="ForteBank RAG Chatbot",
    page_icon="ğŸ¦",
    layout="wide",
)

# â”€â”€â”€ ENVIRONMENT â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Load local .env if present
load_dotenv()
OPENAI_API_KEY = st.secrets.get("OPENAI_API_KEY", os.getenv("OPENAI_API_KEY"))
if not OPENAI_API_KEY:
    st.error("âŒ OPENAI_API_KEY Ğ½Ğµ Ğ·Ğ°Ğ´Ğ°Ğ½Ğ°.")
    st.stop()

DATA_PATH = "./knowledge_base"


# â”€â”€â”€ RAG PROMPT TEMPLATE â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
RAG_TEMPLATE = """Ğ¢Ñ‹ - Ğ´Ñ€ÑƒĞ¶ĞµĞ»ÑĞ±Ğ½Ñ‹Ğ¹ Ğ¸ Ğ¿Ñ€Ğ¾Ñ„ĞµÑÑĞ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ°ÑÑĞ¸ÑÑ‚ĞµĞ½Ñ‚ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ğ»Ğ°Ñ‚ĞµĞ¶Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼ ForteBank. Ğš Ñ‚ĞµĞ±Ğµ Ğ¾Ğ±Ñ€Ğ°Ñ‰Ğ°ÑÑ‚ÑÑ Ñ Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ñ„Ğ¸Ğ»Ğ¸Ğ°Ğ»Ğ¾Ğ² Ğ±Ğ°Ğ½ĞºĞ° Ñ Ñ€Ğ°Ğ·Ğ½Ñ‹Ğ¼Ğ¸ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ°Ğ¼Ğ¸, Ñ‚Ñ‹ Ğ´Ğ¾Ğ»Ğ¶ĞµĞ½ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ñ‚ÑŒ Ğ·Ğ°Ğ¿Ñ€Ğ¾Ñ, ÑÑ‚Ñ€Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¸Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ÑÑÑŒ ÑƒÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹.
    Ğ”Ğ°Ğ¹ Ğ¾Ñ‚Ğ²ĞµÑ‚ Ğ½Ğ° Ğ²Ğ¾Ğ¿Ñ€Ğ¾Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑĞ»ĞµĞ´ÑƒÑÑ‰ĞµĞ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°: 
    {context}

    # Ğ˜Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸
                - Ğ’ÑĞµ Ñ‚Ğ²Ğ¾Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ñ‹ Ğ¾Ğ±ÑĞ·Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ´Ğ¾Ğ»Ğ¶Ğ½Ñ‹ Ğ±Ñ‹Ñ‚ÑŒ Ğ½Ğ° Ñ€ÑƒÑÑĞºĞ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ
                - ĞĞµ Ğ¿Ğ¸ÑˆĞ¸ "Ğ¾Ñ‚Ğ²ĞµÑ‚" Ğ¸Ğ»Ğ¸ Ğ´Ñ€ÑƒĞ³Ğ¸Ğµ ÑĞ»Ğ¾Ğ²Ğ° Ğ¿ĞµÑ€ĞµĞ´ ÑĞ°Ğ¼Ğ¸Ğ¼ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ¼, ÑÑ€Ğ°Ğ·Ñƒ Ğ¿Ğ¸ÑˆĞ¸ ÑĞ°Ğ¼ Ğ¾Ñ‚Ğ²ĞµÑ‚
                - ĞŸĞ¾Ğ·Ğ´Ğ°Ñ€Ğ¾Ğ²Ğ°Ğ¹ÑÑ Ñ ÑĞ¾Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¸ĞºĞ¾Ğ¼ Ñ„Ğ¸Ğ»Ğ¸Ğ°Ğ»Ğ° Ğ² Ğ´Ğ¾Ğ±Ñ€Ğ¾Ğ¶ĞµĞ»Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ¼Ğ°Ğ½ĞµÑ€Ğµ
                - ĞŸĞµÑ€ĞµĞ´ Ñ‚ĞµĞ¼ ĞºĞ°Ğº Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¸Ñ‚ÑŒ Ğ½Ğ° Ğ²Ğ¾Ğ¿Ñ€Ğ¾Ñ ÑĞ¾Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¸ĞºĞ° - Ğ¿Ñ€Ğ¾Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞ¹ Ğ²Ğ¾Ğ¿Ñ€Ğ¾Ñ Ğ¸ ÑÑ€Ğ°Ğ²Ğ½Ğ¸ Ñ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼.
                    - Ğ•ÑĞ»Ğ¸ Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ Ğ¿Ñ€Ğ¸ÑÑƒÑ‚ÑÑ‚Ğ²ÑƒĞµÑ‚ Ğ¾Ñ‚Ğ²ĞµÑ‚ Ğ½Ğ° Ğ²Ğ¾Ğ¿Ñ€Ğ¾Ñ ĞºĞ»Ğ¸ĞµĞ½Ñ‚Ğ°, Ñ‚Ğ¾ Ğ»ÑĞ±ĞµĞ·Ğ½Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²ÑŒ ĞµĞ³Ğ¾.
                    - Ğ•ÑĞ»Ğ¸ Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ ĞµÑÑ‚ÑŒ ÑÑ…Ğ¾Ğ¶Ğ°Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ½Ğ° Ñ‚ĞµĞ¼Ñƒ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ° ĞºĞ»Ğ¸ĞµĞ½Ñ‚Ğ°, Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²ÑŒ ĞµÑ‘ Ğ¸ ÑƒÑ‚Ğ¾Ñ‡Ğ½Ğ¸, ÑÑ‚Ğ¾Ñ‚ Ğ»Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚ Ñ…Ğ¾Ñ‚ĞµĞ» ĞºĞ»Ğ¸ĞµĞ½Ñ‚.
                    - Ğ•ÑĞ»Ğ¸ Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ Ğ¾Ñ‚ÑÑƒÑ‚ÑÑ‚Ğ²ÑƒĞµÑ‚ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ ÑÑ…Ğ¾Ğ¶Ğ°Ñ Ñ Ñ‚ĞµĞ¼Ğ¾Ğ¹ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ° ĞºĞ»Ğ¸ĞµĞ½Ñ‚Ğ°, Ñ‚Ğ¾ Ğ¾Ñ‚Ğ²ĞµÑ‚ÑŒ ĞºĞ°Ğº Ğ² Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ°Ñ… Ñ„Ñ€Ğ°Ğ· Ğ½Ğ¸Ğ¶Ğµ.
                - Ğ’ĞµĞ¶Ğ»Ğ¸Ğ²Ğ¾ Ğ¾Ñ‚ĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ğ¹ Ğ² ÑĞ»ĞµĞ´ÑƒÑÑ‰Ğ¸Ñ… ÑĞ»ÑƒÑ‡Ğ°ÑÑ…:
                    - Ğ•ÑĞ»Ğ¸ Ğ²Ğ¾Ğ¿Ñ€Ğ¾Ñ ÑĞ¾Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¸ĞºĞ° ĞºĞ°ÑĞ°ĞµÑ‚ÑÑ Ñ‚ĞµĞ¼, Ğ½ĞµÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ğ¿Ğ»Ğ°Ñ‚ĞµĞ¶Ğ½Ñ‹Ğ¼Ğ¸ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ğ¼Ğ¸.
                    - Ğ•ÑĞ»Ğ¸ Ğ²Ğ¾Ğ¿Ñ€Ğ¾Ñ ÑĞ¾Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¸ĞºĞ° ĞºĞ°ÑĞ°ĞµÑ‚ÑÑ Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¸Ğ»Ğ¸ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ° Ğ¸Ğ»Ğ¸ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ğ½Ğ¸Ñ Ñ‚Ğ²Ğ¾ĞµĞ³Ğ¾ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ°.
                    - Ğ•ÑĞ»Ğ¸ ĞºĞ»Ğ¸ĞµĞ½Ñ‚ ÑƒÑ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ² Ñ‚Ğ²Ğ¾ĞµĞ¼ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğµ Ğ¾ÑˆĞ¸Ğ±ĞºĞ°.
                - Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ÑĞ¹ Ğ¿Ñ€Ğ¾Ñ„ĞµÑÑĞ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹, Ğ´Ñ€ÑƒĞ¶ĞµĞ»ÑĞ±Ğ½Ñ‹Ğ¹ Ñ‚Ğ¾Ğ½ Ğ²Ğ¾ Ğ²ÑĞµÑ… Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ°Ñ….
                - Ğ¡Ñ‚Ğ°Ñ€Ğ°Ğ¹ÑÑ Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ Ğ¼Ğ°ĞºÑĞ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ Ğ¸ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¾Ñ‚Ğ²ĞµÑ‚Ñ‹

                # ĞŸÑ€Ğ¸Ğ¼ĞµÑ€Ñ‹ Ñ„Ñ€Ğ°Ğ·
                ## ĞÑ‚ÑÑƒÑ‚ÑÑ‚Ğ²Ğ¸Ğµ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ
                - Ğ¯ Ğ½Ğµ Ñ€Ğ°ÑĞ¿Ğ¾Ğ»Ğ°Ğ³Ğ°Ñ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ ÑĞ²ĞµĞ´ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ¿Ğ¾ ÑÑ‚Ğ¾Ğ¼Ñƒ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑƒ. 
                - Ğ­Ñ‚Ğ¾Ñ‚ Ğ²Ğ¾Ğ¿Ñ€Ğ¾Ñ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ·Ğ° Ñ€Ğ°Ğ¼ĞºĞ¸ Ğ¼Ğ¾Ğ¸Ñ… Ñ‚ĞµĞºÑƒÑ‰Ğ¸Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹.  
                - ĞœĞ½Ğµ Ğ¶Ğ°Ğ»ÑŒ, Ğ½Ğ¾ Ñ Ğ½Ğµ Ğ¼Ğ¾Ğ³Ñƒ Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ¸Ñ‚ÑŒ Ğ²Ğ°Ğ¼ ÑÑ‚Ñƒ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ. 

                ## ĞÑ‚ĞºĞ»Ğ¾Ğ½ĞµĞ½Ğ¸Ğµ Ğ·Ğ°Ğ¿Ñ€ĞµÑ‰ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¸Ğ»Ğ¸ Ğ½ĞµÑ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ğ¾Ğ¹ Ñ‚ĞµĞ¼Ñ‹
                - Â«ĞœĞ½Ğµ Ğ¾Ñ‡ĞµĞ½ÑŒ Ğ¶Ğ°Ğ»ÑŒ, Ğ½Ğ¾ Ñ Ğ½Ğµ Ğ¼Ğ¾Ğ³Ñƒ Ğ¾Ğ±ÑÑƒĞ¶Ğ´Ğ°Ñ‚ÑŒ ÑÑ‚Ñƒ Ñ‚ĞµĞ¼Ñƒ. ĞœĞ¾Ğ¶ĞµÑ‚ Ğ±Ñ‹Ñ‚ÑŒ, Ñ Ğ¼Ğ¾Ğ³Ñƒ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‡ÑŒ Ğ²Ğ°Ğ¼ Ğ² Ñ‡ĞµĞ¼-Ñ‚Ğ¾ Ğ´Ñ€ÑƒĞ³Ğ¾Ğ¼?Â»
                - Â«Ğ¯ Ğ½Ğµ Ğ¼Ğ¾Ğ³Ñƒ Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ¸Ñ‚ÑŒ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¿Ğ¾ ÑÑ‚Ğ¾Ğ¼Ñƒ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑƒ, Ğ½Ğ¾ Ñ Ğ±ÑƒĞ´Ñƒ Ñ€Ğ°Ğ´ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‡ÑŒ Ğ²Ğ°Ğ¼ Ñ Ğ»ÑĞ±Ñ‹Ğ¼Ğ¸ Ğ´Ñ€ÑƒĞ³Ğ¸Ğ¼Ğ¸ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ°Ğ¼Ğ¸Â».

    Ğ’Ğ¾Ğ¿Ñ€Ğ¾Ñ: {question}
    """
# â”€â”€â”€ CACHED RESOURCES â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@st.cache_resource
def get_embedding_function():
    return OpenAIEmbeddings(
        model="text-embedding-3-small",
        dimensions=1536,
        openai_api_key=OPENAI_API_KEY
    )
    

@st.cache_resource
def build_vector_store(_docs):
    splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
    chunks   = splitter.split_documents(_docs)
    embed_fn = get_embedding_function()
    settings = Settings(
        chroma_db_impl="duckdb+parquet",
        # persist_directory can be None or a temp dir; 
        # itâ€™ll live in-memory for this session either way
        persist_directory=None,
    )
    return Chroma.from_documents(
        documents=chunks,
        embedding=embed_fn,
        client_settings=settings,
        persist_directory=None,   # in-RAM only
    )

@st.cache_resource(hash_funcs={Chroma: lambda _: None})
def create_rag_chain(vector_store, model_name, temperature):
    llm = ChatOpenAI(
        model=model_name,
        temperature=temperature,
        openai_api_key=OPENAI_API_KEY
    )
    retriever = vector_store.as_retriever(
        search_type="similarity",
        search_kwargs={"k": 3}
    )
    prompt = ChatPromptTemplate.from_template(RAG_TEMPLATE)
    return (
        {"context": retriever, "question": RunnablePassthrough()}
        | prompt
        | llm
        | StrOutputParser()
    )

# â”€â”€â”€ HELPERS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def load_local_docs():
    """Load all PDFs from your committed folder."""
    paths = glob.glob(os.path.join(DATA_PATH, "**/*.pdf"), recursive=True)
    docs = []
    for p in paths:
        docs.extend(PyPDFLoader(p).load())
    return docs

# â”€â”€â”€ MAIN â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def main():
    st.title("ğŸ¦ ForteBank RAG Chatbot")
    st.markdown("Ğ—Ğ°Ğ´Ğ°Ğ¹Ñ‚Ğµ Ğ²Ğ¾Ğ¿Ñ€Ğ¾Ñ Ğ¿Ğ¾ Ğ¿Ğ»Ğ°Ñ‚ĞµĞ¶Ğ½Ñ‹Ğ¼ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ğ¼ ForteBank.")

    # â”€ Sidebar â”€
    st.sidebar.header("ğŸ“¤ Ğ—Ğ°Ğ³Ñ€ÑƒĞ·Ğ¸Ñ‚ÑŒ PDF")
    uploaded = st.sidebar.file_uploader(
        "Ğ’Ñ‹Ğ±ĞµÑ€Ğ¸Ñ‚Ğµ PDF-Ñ„Ğ°Ğ¹Ğ»Ñ‹",
        type=["pdf"],
        accept_multiple_files=True,
        key="pdf_uploader"
    )

    if uploaded and not st.session_state.get("upload_processed", False):
        # Load newly uploaded docs
        new_docs = []
        for f in uploaded:
            new_docs.extend(PyPDFLoader(f).load())

        # Combine with committed docs
        combined_docs = load_local_docs() + new_docs

        # Rebuild vector store in RAM
        st.session_state.vector_store = build_vector_store(combined_docs)
        st.session_state.upload_processed = True
        st.sidebar.success("âœ… Ğ—Ğ°Ğ³Ñ€ÑƒĞ¶ĞµĞ½Ğ¾ Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ½Ğ´ĞµĞºÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¾!")

    # â”€ Initialize vector store on first run â”€
    if "vector_store" not in st.session_state:
        st.session_state.vector_store = build_vector_store(load_local_docs())

    # â”€ Build the RAG chain â”€
    vector_store = st.session_state.vector_store
    rag_chain = create_rag_chain(vector_store, model_option, temperature)

    # â”€ Chat UI â”€
    if "messages" not in st.session_state:
        st.session_state.messages = []

    # Display history
    for msg in st.session_state.messages:
        with st.chat_message(msg["role"]):
            st.markdown(msg["content"])

    # Input
    if prompt := st.chat_input("Ğ’Ğ°Ñˆ Ğ²Ğ¾Ğ¿Ñ€Ğ¾Ñ..."):
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)

        with st.chat_message("assistant"):
            with st.spinner("ĞĞ±Ğ´ÑƒĞ¼Ñ‹Ğ²Ğ°Ñ Ğ¾Ñ‚Ğ²ĞµÑ‚â€¦"):
                try:
                    answer = rag_chain.invoke(prompt)
                except Exception as e:
                    answer = f"ĞÑˆĞ¸Ğ±ĞºĞ°: {e}"
                st.markdown(answer)
                st.session_state.messages.append({
                    "role": "assistant",
                    "content": answer
                })

    # â”€ Clear chat â”€
    if st.button("ğŸ—‘ï¸ ĞÑ‡Ğ¸ÑÑ‚Ğ¸Ñ‚ÑŒ Ñ‡Ğ°Ñ‚"):
        st.session_state.messages = []

if __name__ == "__main__":
    main()
